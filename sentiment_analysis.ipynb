{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis - Tokenizing news headlines for data preparation!\n",
    "The notebook covers the data preparation step by tokenizing the headlines and creating padded sequences of news headlines.\n",
    "\n",
    "Data preparation include the following steps:\n",
    "1. Download and read the data\n",
    "2. Segregate the headlines and their labels.\n",
    "3. Tokenize the headlines\n",
    "4. Create sequences and add padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download and read the news headlines data\n",
    "\n",
    "This is a [kaggle dataset](https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection) which is further corrected and then hosted on Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/wdd-2-node.appspot.com/x1.json \\\n",
    "    -o /tmp/headlines.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##read the data using the pandas library\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json(\"./x1.json\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segregating the headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create lists to store the headlines and labels\n",
    "headlines = list(data['headline'])\n",
    "labels = list(data['is_sarcastic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import the required APIs\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##set up the tokenizer\n",
    "tokenizer = Tokenizer(oov_token=\"<oov>\")\n",
    "tokenizer.fit_on_texts(headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(word_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create padded sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create sequences of the headlines\n",
    "seqs = tokenizer.texts_to_sequences(headlines)\n",
    "\n",
    "##post-pad sequences\n",
    "padded_seqs = pad_sequences(seqs, padding=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##printing padded sequences sample\n",
    "print(padded_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
